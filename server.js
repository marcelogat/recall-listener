const WebSocket = require('ws');
const { createClient } = require('@supabase/supabase-js');
const { ThinkingAgent } = require('./thinking-agent');

// --- CONFIGURACIÃ“N ---
const supabaseUrl = process.env.SUPABASE_URL;
const supabaseKey = process.env.SUPABASE_ANON_KEY;
const supabase = createClient(supabaseUrl, supabaseKey);

const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
const RECALL_API_KEY = process.env.RECALL_API_KEY;
const RECALL_REGION = process.env.RECALL_REGION || 'us-west-2';
const ELEVENLABS_API_KEY = process.env.ELEVENLABS_API_KEY;

const wss = new WebSocket.Server({ port: 8080 });

console.log('ðŸš€ Servidor WebSocket iniciado en puerto 8080');

// --- FUNCIONES DE CARGA ---
async function loadActiveAgent(agentName = null) {
  // (Misma lÃ³gica que tenÃ­as antes, funciona bien)
  let query = supabase.from('agents').select(`*, agent_voice_config (*)`).eq('is_active', true);
  
  if (agentName) {
    query = query.eq('name', agentName.toLowerCase());
  } else {
    query = query.eq('is_default', true);
  }

  const { data: agent, error } = await query.single();
  if (error || !agent) throw new Error(`Error cargando agente: ${error?.message}`);

  const voiceConfig = agent.agent_voice_config.find(v => v.is_active);
  if (!voiceConfig) throw new Error(`Agente sin configuraciÃ³n de voz activa`);

  console.log(`âœ… Agente cargado: ${agent.display_name} (${agent.llm_model})`);
  return { agent, voiceConfig };
}

// --- WEBSOCKET HANDLER ---
wss.on('connection', async function connection(ws, req) {
  const clientIp = req.socket.remoteAddress;
  console.log(`\nðŸ”Œ Nueva conexiÃ³n desde: ${clientIp}`);

  let agentConfig;
  try {
    const url = new URL(req.url, `http://${req.headers.host}`);
    const agentName = url.searchParams.get('agent');
    agentConfig = await loadActiveAgent(agentName);
  } catch (error) {
    console.error('âŒ Error fatal:', error.message);
    ws.close(1011, error.message);
    return;
  }

  const { agent, voiceConfig } = agentConfig;
  const meetingId = `meeting_${Date.now()}_${Math.random().toString(36).substr(2, 5)}`;
  
  // ðŸ§  INICIALIZAR CEREBRO
  // Usamos la versiÃ³n mejorada de ThinkingAgent (asegÃºrate de actualizar ese archivo tambiÃ©n)
  const thinkingAgent = new ThinkingAgent(meetingId, agentConfig);

  // CONFIGURACIÃ“N DE TIEMPOS
  const SILENCE_TIMEOUT = agent.silence_timeout_ms || 1500; // Tiempo de espera tras dejar de hablar
  const MAX_CONTEXT_HISTORY = 15; 

  // ESTADO DE LA CONVERSACIÃ“N
  let conversationHistory = [];
  let currentUtterance = [];
  let silenceTimeoutId = null;
  let lastWordTime = Date.now();
  let botId = null;
  
  // ESTADO DE INTERACCIÃ“N
  let isAgentSpeaking = false;
  let isProcessingResponse = false;
  let shouldCancelResponse = false; // Flag para interrupciones

  // --- FUNCIONES CORE ---

  async function generateElevenLabsAudio(text) {
    try {
      const response = await fetch(`https://api.elevenlabs.io/v1/text-to-speech/${voiceConfig.voice_id}`, {
        method: 'POST',
        headers: {
          'Accept': 'audio/mpeg',
          'xi-api-key': ELEVENLABS_API_KEY,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          text: text,
          model_id: voiceConfig.voice_model,
          voice_settings: voiceConfig.voice_settings
        })
      });

      if (!response.ok) throw new Error(`ElevenLabs: ${response.statusText}`);
      const arrayBuffer = await response.arrayBuffer();
      return Buffer.from(arrayBuffer).toString('base64');
    } catch (e) {
      console.error('âŒ Error TTS:', e.message);
      return null;
    }
  }

  async function sendAudioToRecall(audioBase64) {
    if (!botId) return;
    try {
      await fetch(`https://${RECALL_REGION}.recall.ai/api/v1/bot/${botId}/output_audio/`, {
        method: 'POST',
        headers: {
          'Authorization': `Token ${RECALL_API_KEY}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({ kind: 'mp3', b64_data: audioBase64 })
      });
    } catch (e) {
      console.error('âŒ Error enviando audio a Recall:', e.message);
    }
  }

  /**
   * âš¡ CORE LOGIC: Unifica decisiÃ³n y generaciÃ³n para latencia mÃ­nima.
   * El Prompt instruye al modelo a devolver "[SILENCE]" si no debe hablar.
   */
  async function processAndRespond(userText, speakerName) {
    if (isProcessingResponse) return;
    
    isProcessingResponse = true;
    shouldCancelResponse = false; // Nuevo ciclo

    try {
      // 1. Disparar "Pensamiento" en background (Fire & Forget)
      // No usamos 'await' para no bloquear la voz
      thinkingAgent.processUtterance(userText, {
        speakerName,
        speakerId: speakerName, // Idealmente usar ID real si existe
        isAgentSpeaking: false
      }).catch(err => console.error('Error en thinkingAgent:', err));

      console.log(`\nðŸ“¨ Procesando: "${userText}"`);

      // 2. Construir historial para contexto
      const recentHistory = conversationHistory.map(m => 
        `${m.role === 'user' ? `(${m.speaker})` : '(Assistant)'}: ${m.content}`
      ).join('\n');

      const systemPrompt = `${agent.profile_text}
      
      INSTRUCCIONES DE COMPORTAMIENTO EN TIEMPO REAL:
      1. Eres un participante mÃ¡s en la reuniÃ³n.
      2. Si el usuario NO te estÃ¡ hablando a ti, o estÃ¡n hablando entre ellos, o solo dijo algo corto como "ok" o "gracias", TU RESPUESTA DEBE SER: [SILENCE]
      3. Si debes responder, sÃ© conciso, natural y directo.
      4. No saludes todo el tiempo.
      
      IMPORTANTE: Responde SOLAMENTE con el texto que vas a decir, o "[SILENCE]" si decides callar.`;

      // 3. Llamada optimizada a OpenAI
      const completion = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${OPENAI_API_KEY}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: agent.llm_model || 'gpt-4o', // Usar modelo rÃ¡pido
          messages: [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: `CONTEXTO PREVIO:\n${recentHistory}\n\nAHORA:\n[${speakerName}]: ${userText}` }
          ],
          temperature: 0.6,
          max_tokens: 150 // Limitar longitud para velocidad
        })
      });

      const data = await completion.json();
      let aiResponse = data.choices?.[0]?.message?.content || '[SILENCE]';

      // 4. Verificar cancelaciÃ³n por interrupciÃ³n (Barge-in)
      if (shouldCancelResponse) {
        console.log('ðŸ›‘ Procesamiento cancelado: Usuario interrumpiÃ³.');
        return;
      }

      // 5. Evaluar decisiÃ³n del modelo
      if (aiResponse.includes('[SILENCE]')) {
        console.log('ðŸ¤« El bot decidiÃ³ callar.');
        // Guardamos en historial que escuchamos, pero no respondimos
        conversationHistory.push({ role: 'user', content: userText, speaker: speakerName });
        return;
      }

      console.log(`ðŸ—£ï¸ Bot va a decir: "${aiResponse}"`);

      // 6. Generar Audio (TTS)
      const audioBase64 = await generateElevenLabsAudio(aiResponse);
      
      if (audioBase64 && !shouldCancelResponse) {
        isAgentSpeaking = true;
        await sendAudioToRecall(audioBase64);
        
        // Actualizar historial
        conversationHistory.push({ role: 'user', content: userText, speaker: speakerName });
        conversationHistory.push({ role: 'assistant', content: aiResponse });
        
        // Limitar historial
        if (conversationHistory.length > MAX_CONTEXT_HISTORY) {
          conversationHistory = conversationHistory.slice(-MAX_CONTEXT_HISTORY);
        }

        // Notificar al agente pensante que hablamos
        thinkingAgent.processUtterance(aiResponse, {
          speakerName: agent.display_name,
          speakerId: 'agent',
          isAgentSpeaking: true
        });

        // Timer para "soltar" el flag de speaking (estimado)
        setTimeout(() => { isAgentSpeaking = false; }, (aiResponse.length * 80)); 
      }

    } catch (error) {
      console.error('âŒ Error procesando respuesta:', error);
    } finally {
      isProcessingResponse = false;
    }
  }

  // --- MANEJO DE WEBSOCKET ---

  ws.on('message', async function incoming(message) {
    try {
      const data = JSON.parse(message);

      // Capturar Bot ID
      if (!botId && data.data?.bot?.id) {
        botId = data.data.bot.id;
        console.log(`ðŸ¤– Bot ID vinculado: ${botId}`);
      }

      // A. DATOS PARCIALES (Usuario estÃ¡ hablando ahora mismo)
      if (data.event === 'transcript.partial_data') {
        const words = data.data?.data?.words;
        if (words && words.length > 0) {
          lastWordTime = Date.now();
          
          // LÃ“GICA DE INTERRUPCIÃ“N (BARGE-IN)
          if (isAgentSpeaking || isProcessingResponse) {
            console.log('â— InterrupciÃ³n detectada!');
            shouldCancelResponse = true; // Cancela proceso LLM/TTS
            isAgentSpeaking = false;
            // Opcional: Enviar seÃ±al de stop a Recall si soportan endpoint de "Clear Buffer"
          }
          
          // Reiniciar timer de silencio (Keep-alive del turno del usuario)
          if (silenceTimeoutId) clearTimeout(silenceTimeoutId);
          
          // Re-crear el timer
          silenceTimeoutId = setTimeout(processAccumulatedAudio, SILENCE_TIMEOUT);
        }
      }

      // B. DATOS CONFIRMADOS (Bloque de texto finalizado)
      if (data.event === 'transcript.data') {
        const words = data.data?.data?.words;
        const participant = data.data?.data?.participant;
        
        if (words && words.length > 0) {
          lastWordTime = Date.now();
          if (silenceTimeoutId) clearTimeout(silenceTimeoutId);

          const speakerName = participant?.name || 'Usuario';
          
          // Acumular palabras en el buffer actual
          words.forEach(w => {
            currentUtterance.push({ 
              text: w.text, 
              speakerName: speakerName 
            });
          });

          // Reiniciar timer para procesar el bloque completo
          silenceTimeoutId = setTimeout(processAccumulatedAudio, SILENCE_TIMEOUT);
        }
      }

    } catch (e) {
      console.error('Error WS parsing:', e);
    }
  });

  // FunciÃ³n que se ejecuta cuando hay SILENCIO tras hablar
  async function processAccumulatedAudio() {
    if (currentUtterance.length === 0) return;

    // Reconstruir frase
    const fullText = currentUtterance.map(w => w.text).join(' ').trim();
    const speakerName = currentUtterance[0].speakerName;
    
    // Limpiar buffer para la prÃ³xima
    currentUtterance = [];

    // Validaciones bÃ¡sicas
    if (fullText.length < 3) return; // Ignorar ruidos muy cortos
    
    console.log(`\nðŸ“ Transcript Final (${speakerName}): "${fullText}"`);

    // Enviar al nÃºcleo de IA
    await processAndRespond(fullText, speakerName);
  }

  ws.on('close', async () => {
    console.log('ðŸ”Œ ConexiÃ³n cerrada');
    if (silenceTimeoutId) clearTimeout(silenceTimeoutId);
    
    // Generar reporte final del ThinkingAgent
    await thinkingAgent.getFinalThoughts();
  });
});
